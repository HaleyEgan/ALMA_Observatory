{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Classifier for Software & Hardware Jira Summary Tickets\n",
    "- PRTSIR = Hardware = 1\n",
    "- ICT = Software = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe inputs\n",
    "# Dataset pharmacy_dataset_reduced renamed to pharmacy_dataset_TEST by vkb6bn on 2023-03-08 17:42:52\n",
    "data = dataiku.Dataset(\"PRTSIR_TICKETS_dataset_stacked\")\n",
    "data = data.get_dataframe()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any missing values\n",
    "data = data.dropna()\n",
    "#reset index\n",
    "data=data.reset_index(drop=True)\n",
    "#shuffle data\n",
    "data = shuffle(data)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of unique classes under 'label'\n",
    "num_classes=len(data.label.unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y variable\n",
    "y = tf.keras.utils.to_categorical(data[\"label\"].values, num_classes=num_classes)\n",
    "#y\n",
    "\n",
    "#split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'], y, test_size=0.3)\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bert with tensorflow hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
    "\n",
    "#function to get word embeddings\n",
    "def get_embeddings(sentences):\n",
    "  '''return BERT-like embeddings of input text\n",
    "  Args:\n",
    "    - sentences: list of strings\n",
    "  Output:\n",
    "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
    "  '''\n",
    "  preprocessed_text = preprocessor(sentences)\n",
    "  return encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "#test function\n",
    "#get_embeddings([\n",
    "   # \"This is a test para ver el futuro of the model.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resource: https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d#98ee\n",
    "from keras import backend as K\n",
    "\n",
    "#functions to find recall\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "#functions to find precision\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "#functions to find f1 score\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "x = preprocessor(i)\n",
    "x = encoder(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
    "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
    "\n",
    "bert_model = tf.keras.Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of epochs\n",
    "n_epochs = 10\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "      tf.keras.metrics.TrueNegatives(),\n",
    "      tf.keras.metrics.FalsePositives(),\n",
    "      tf.keras.metrics.AUC(),\n",
    "      balanced_recall,\n",
    "      balanced_precision,\n",
    "      balanced_f1_score\n",
    "]\n",
    "\n",
    "#EarlyStopping callback to monitor validation loss\n",
    "#if metric doesn't improve for at least 3 epochs (patience = 3)\n",
    "    #training is interrupted and weights from epoch where the validation loss\n",
    "    #showed the best value (i.e. lowest) are restored (restore_best_weights = True)\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
    "                                                      patience = 3,\n",
    "                                                      restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "bert_model.compile(optimizer = \"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "model_fit = bert_model.fit(x_train,\n",
    "                      y_train,\n",
    "                      epochs = n_epochs,\n",
    "                      validation_data = (x_test, y_test),\n",
    "                      callbacks = [earlystop_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = list(range(1, n_epochs+1))\n",
    "metric_list = list(model_fit.history.keys())\n",
    "num_metrics = int(len(metric_list)/2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
    "\n",
    "for i in range(0, num_metrics):\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
    "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
    "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
    "  ax[i].legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text from Description Column\n",
    "ICT_text = [\"aca control operations software support inform operator time restart bl correlator acacorr observation runinng new instance ict-4841 not find channel average datum inspect log last month corroborate little ict-4841 relate bl restart common point bl aca gns network indeed bl machine reinitializing get image gns:/tftpboot/ aca node show log slow cdp transmission aca master example code 21t09:45:10.427 acc mastercontainer corr_master_comp shutdownsubsyspass1 method call /var log message cob cpn-05 syslogd restart remote reception cob cpn-14 syslogd restart remote reception cob cpn-03 syslogd restart remote reception cob cpn-13 syslogd restart remote reception cob cc syslogd restart remote reception cob cpn-07 syslogd restart remote reception cob cpn-15 syslogd restart remote reception cob cpn-04 syslogd restart remote reception cob cpn-01 syslogd restart remote reception cob cpn-02 syslogd restart remote reception cob cpn-16 syslogd restart remote reception cob cpn-06 syslogd restart remote reception cob cpn-09 syslogd restart remote reception cob cpn-12 syslogd restart remote reception cob cpn-08 syslogd restart remote reception cob cpn-11 syslogd restart remote reception cob cpn-10 syslogd restart remote reception 21t09:51:44.433 acacorr cdpmif cppcontainer acacorr cdpmif master not find channel average datum key[obs/0000002799/2/1 nid[9 file[acacdpmchanaverdataprocthread.cpp line[357 21t09:52:46.999 acacorr cdpmif cppcontainer acacorr cdpmif master not find channel average datum key[obs/0000002818/3/5 nid[5 file[acacdpmchanaverdataprocthread.cpp line[357 21t09:52:47.389 acacorr cdpc_data_mgr n07 cppcontainer acacorr cdpc_data_mgr node_07 cdp datum transmisson long elaps[4.138645(sp=0.000001/0.063965,ch=4.138644/4.138635 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:47.665 acacorr cdpc_data_mgr n32 cppcontainer acacorr cdpc_data_mgr node_32 cdp datum transmisson long elaps[4.418394(sp=0.000001/0.082536,ch=4.418393/4.418384 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:47.849 acacorr cdpc_data_mgr n13 cppcontainer acacorr cdpc_data_mgr node_13 cdp datum transmisson long elaps[4.599679(sp=0.000001/0.086688,ch=4.599678/4.599669 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:48.045 acacorr cdpc_data_mgr n30 cppcontainer acacorr cdpc_data_mgr node_30 cdp datum transmisson long elaps[4.806524(sp=0.000001/0.083923,ch=4.806523/4.806515 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:49.003 acacorr cdpc_data_mgr n22 cppcontainer acacorr cdpc_data_mgr node_22 cdp datum transmisson long elaps[5.766346(sp=0.000001/0.060834,ch=5.766345/5.766336 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:49.003 acacorr cdpc_data_mgr n20 cppcontainer acacorr cdpc_data_mgr node_20 cdp datum transmisson long elaps[5.764070(sp=0.000001/0.039338,ch=5.764069/5.764060 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:49.003 acacorr cdpc_data_mgr n21 cppcontainer acacorr cdpc_data_mgr node_21 cdp datum transmisson long elaps[5.759761(sp=0.000001/0.086281,ch=5.759760/5.759751 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:49.027 acacorr cdpc_data_mgr n06 cppcontainer acacorr cdpc_data_mgr node_06 cdp datum transmisson long elaps[5.771451(sp=0.000001/0.061036,ch=5.771450/5.771441 file[acacdpcdatapublisherthread.cpp line[258 21t09:52:49.707 acacorr cdpc_data_mgr n29 cppcontainer acacorr cdpc_data_mgr node_29 cdp datum transmisson long elaps[5.457301(sp=0.000001/0.058710,ch=5.457300/5.457292 file[acacdpcdatapublisherthread.cpp line[258 code assign ticket initially softops order deeply understand relation apparently separate subsytems bl corr restart time aca cdp datum transmisson long\"]\n",
    "PRTSIR_text = [\"Focus SBs BL correction Version 0 failed project started at 3 PM and ended at 3 06 PM project code 0000 0 00201 CSV PI nphillip SchedBlock focused on Band 3 Z ExecBlock has SB UID QA0 status system part of ALMA RB 03 band built on December 22 2014 list of devices connected to system including channel and node information some devices include LO Reference Receiver IF Processor Digitizer Clock and 2nd LO Synthesizer attempt made to turn on FEPS device for antenna DV22 but failed device state changed from Stop to Start but encountered error in process device could not go to Configure State possibly because not responding to SN CanBus request as result antenna out of array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict class function\n",
    "def predict_class(test_text):\n",
    "  '''predict class of input text\n",
    "  Args:\n",
    "    - comments (list of strings)\n",
    "  Output:\n",
    "    - class (list of int)\n",
    "  '''\n",
    "  return [np.argmax(pred) for pred in bert_model.predict(test_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(ICT_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(PRTSIR_text)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_BERT_Summary_Text",
  "createdOn": 1679017553074,
  "creationTag": {
   "lastModifiedBy": {
    "login": "vkb6bn"
   },
   "lastModifiedOn": 1679017553074,
   "versionNumber": 0
  },
  "creator": "vkb6bn",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "modifiedBy": "vkb6bn",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
