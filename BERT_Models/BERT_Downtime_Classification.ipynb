{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model to Classify Downtimes by Subject/Comments - Multi-Class Classification\n",
    "- kernel = env mypython\n",
    "- text cleaning / preparation done before this step\n",
    "- Resources: https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe inputs\n",
    "data = dataiku.Dataset(\"DowntimeSubjects_prepared\")\n",
    "data = data.get_dataframe()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "data=data.reset_index(drop=True) \n",
    "#shuffle data\n",
    "data = shuffle(data)   \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of unique classes under 'label'\n",
    "num_classes=len(data.DOWNTIME_TYPE.unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change label words to numbers: Technical = 0, Weather = 1, Schedulin = 2\n",
    "\n",
    "# Use the loc accessor to replace 'Scheduling' with 2, 'Technical' with 0, and 'Weather' with 1\n",
    "data.loc[data['DOWNTIME_TYPE'] == 'Scheduling', 'DOWNTIME_TYPE'] = 2\n",
    "data.loc[data['DOWNTIME_TYPE'] == 'Technical', 'DOWNTIME_TYPE'] = 0\n",
    "data.loc[data['DOWNTIME_TYPE'] == 'Weather', 'DOWNTIME_TYPE'] = 1\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data - Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y variable\n",
    "y = tf.keras.utils.to_categorical(data[\"DOWNTIME_TYPE\"].values, num_classes=num_classes)\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['SUBJECT_cleaned'], y, test_size=0.3)\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling\n",
    "#### Load BERT with TensorFlow Hub\n",
    "- repository of trained machine learning models\n",
    "- use universal-sentence-encoder-cmlm/multilingual-base = universal sentence encoder (100+ languages)\n",
    "    - uses conditional masked language model\n",
    "- Goal: turn text into high-dim vectors that capture sentence-level semantics\n",
    "    - get embeddings from input text with preprocessor and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bert with tensorflow hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
    "\n",
    "#function to get word embeddings\n",
    "def get_embeddings(sentences):\n",
    "  '''return BERT-like embeddings of input text\n",
    "  Args:\n",
    "    - sentences: list of strings\n",
    "  Output:\n",
    "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
    "  '''\n",
    "  preprocessed_text = preprocessor(sentences)\n",
    "  return encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "#test function\n",
    "#get_embeddings([\n",
    "   # \"This is a test para ver el futuro of the model.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create & Train Classification Model\n",
    "- Observe different metrics during training: Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resource: https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d#98ee\n",
    "from keras import backend as K\n",
    "\n",
    "#functions to find recall\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "#functions to find precision\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "#functions to find f1 score\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model\n",
    "- preporcessor & encoder layers\n",
    "- dropout & dense layer with softmax activation function\n",
    "- output space dimension = # of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='SUBJECT_cleaned')\n",
    "x = preprocessor(i)\n",
    "x = encoder(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
    "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
    "\n",
    "bert_model = tf.keras.Model(i, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile & Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of epochs\n",
    "n_epochs = 1\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "      tf.keras.metrics.TrueNegatives(),\n",
    "      tf.keras.metrics.FalsePositives(),\n",
    "      tf.keras.metrics.AUC(),\n",
    "      balanced_recall,\n",
    "      balanced_precision,\n",
    "      balanced_f1_score\n",
    "]\n",
    "\n",
    "#EarlyStopping callback to monitor validation loss\n",
    "#if metric doesn't improve for at least 3 epochs (patience = 3)\n",
    "    #training is interrupted and weights from epoch where the validation loss \n",
    "    #showed the best value (i.e. lowest) are restored (restore_best_weights = True)\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
    "                                                      patience = 3,\n",
    "                                                      restore_best_weights = True)\n",
    "\n",
    "#compile model\n",
    "bert_model.compile(optimizer = \"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = METRICS)\n",
    "\n",
    "#fit model\n",
    "model = bert_model.fit(x_train, \n",
    "                      y_train, \n",
    "                      epochs = n_epochs,\n",
    "                      validation_data = (x_test, y_test),\n",
    "                      callbacks = [earlystop_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, n_epochs+1))\n",
    "metric_list = list(model.history.keys())\n",
    "num_metrics = int(len(metric_list)/2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
    "\n",
    "for i in range(0, num_metrics):\n",
    "  ax[i].plot(x, model.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
    "  ax[i].plot(x, model.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
    "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
    "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
    "  ax[i].legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Known results - text from dataset\n",
    "- technical (0) - \"pr1 bl aos sbex fail operation timeout\" \n",
    "- technical (0) - \"caimap setting add pms antenna\" \n",
    "- weather (1) - \"snow camera minute far\" \n",
    "- weather (1) - \"no observation pwr very inastable night\" \n",
    "- scheduling (2) - \"waiting min right time start time constrained project 2017.a.00035.s/3fgl_j04_a_06_tm1\" \n",
    "- scheduling (2) - \"nothing dsa observe aca7\" \n",
    "- scheduling (2) - \"no project observe\"\n",
    "\n",
    "##### Known results - easier - text from dataset\n",
    "- weather (1) - \"high wind aos\"\n",
    "- technical (0) - \"bl\"\n",
    "- weather (1) - \"poor weather condition observin\",\n",
    "- technical (0) - \"pr bl aos acs no response container not tell no conn manager\",\n",
    "- scheduling (2) - \"no project\",\n",
    "- scheduling (2) - \"shift ending\",\n",
    "- weather (1) - \"wvr mm high humidity\"\n",
    "\n",
    "##### Unknown results - text from JIRA 'summary' columns \n",
    "- \"aos bl dv17 fe not lock band6\"\n",
    "- \"qa0 runnig aos check aqua produce error\"\n",
    "- \"aos archive subsytem went error\"\n",
    "\n",
    "##### Unknown results - text from JIRA 'description' columns \n",
    "- \"nogo version --success-- start end project code 0000.0.00258.csv pi nphillip schedblock gonogo b3 execblock uid://a002 xaf4574 x3 sb uid uid://a002 x91a40d x5b qa0 status band alma_rb_03 alma build 201508-cycle3-on b-2016 array array001 array corr m]/64-antenna da42 axis go shutdown run sbex no apparent reason recovered send autonomous\"\n",
    "- \"run interactive sb pm01 pm04 suddenly observation crash subscan not end scan checking log find acacorr cmd_transfer cppcontainer gl detected hardware failure\"\n",
    "- \"no way create array antenna steve try create array include da45 container creation give timeout lose available antenna create new array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER: 0, 0, 1, 1, 2, 2, 2\n",
    "known_test_text = [\"pr1 bl aos sbex fail operation timeout\", \n",
    "             \"caimap setting add pms antenna\",\n",
    "             \"snow camera minute far\",\n",
    "             \"no observation pwr very inastable night\",\n",
    "             \"waiting min right time start time constrained project 2017.a.00035.s/3fgl_j04_a_06_tm1\",\n",
    "             \"nothing dsa observe aca7\",\n",
    "             \"no project observe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict class function\n",
    "def predict_class(test_text):\n",
    "  '''predict class of input text\n",
    "  Args:\n",
    "    - comments (list of strings)\n",
    "  Output:\n",
    "    - class (list of int)\n",
    "  '''\n",
    "  return [np.argmax(pred) for pred in bert_model.predict(test_text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(known_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER: 1, 0, 1, 0, 2, 2, 1\n",
    "easier_known_test = [\"high wind aos\",\n",
    "                     \"bl\",\n",
    "                     \"poor weather condition observin\",\n",
    "                     \"pr bl aos acs no response container not tell no conn manager\",\n",
    "                     \"no project\",\n",
    "                     \"shift ending\",\n",
    "                     \"wvr mm high humidity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(easier_known_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_test_text = [\"aos bl dv17 fe not lock band6\",\n",
    "                     \"qa0 runnig aos check aqua produce error\",\n",
    "                     \"aos archive subsytem went error\",\n",
    "                     \"nogo version --success-- start end project code 0000.0.00258.csv pi nphillip schedblock gonogo b3 execblock uid://a002 xaf4574 x3 sb uid uid://a002 x91a40d x5b qa0 status band alma_rb_03 alma build 201508-cycle3-on b-2016 array array001 array corr m]/64-antenna da42 axis go shutdown run sbex no apparent reason recovered send autonomous\",\n",
    "                     \"run interactive sb pm01 pm04 suddenly observation crash subscan not end scan checking log find acacorr cmd_transfer cppcontainer gl detected hardware failure\",\n",
    "                     \"no way create array antenna steve try create array include da45 container creation give timeout lose available antenna create new array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(unknown_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the trained model\n",
    "predictions = bert_model.predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model in Dataiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('DowntimeTypes_BERT_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_BERT_Downtime_Classification",
  "createdOn": 1678311353892,
  "creationTag": {
   "lastModifiedBy": {
    "login": "vkb6bn"
   },
   "lastModifiedOn": 1678311353892,
   "versionNumber": 0
  },
  "creator": "vkb6bn",
  "customFields": {},
  "dkuGit": {
   "gitReference": {
    "checkout": "master",
    "isDirty": false,
    "remote": "git@github.com:HaleyEgan/ALMA_Observatory.git",
    "remotePath": "notebook editor for compute_BERT_Downtime_Classification.ipynb"
   },
   "lastInteraction": 1678839726234
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env mypython)",
   "language": "python",
   "name": "py-dku-venv-mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "modifiedBy": "vkb6bn",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
