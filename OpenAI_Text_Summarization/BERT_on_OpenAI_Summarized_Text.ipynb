{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Classifier on Software vs Hardware Jira Tickets - with description and summary text summarized by OpenAI API\n",
    "- kernel = mypython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "\n",
    "# Read recipe inputs\n",
    "data = dataiku.Dataset(\"OpenAI_API_Text_Summarization\")\n",
    "data = data.get_dataframe()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of unique classes under 'label'\n",
    "num_classes=len(data.label.unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any missing values\n",
    "data = data.dropna()\n",
    "#reset index\n",
    "data=data.reset_index(drop=True)\n",
    "#shuffle data\n",
    "data = shuffle(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y variable\n",
    "y = tf.keras.utils.to_categorical(data[\"label\"].values, num_classes=num_classes)\n",
    "#y\n",
    "\n",
    "#split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['Text_Summarization'], y, test_size=0.3)\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bert with tensorflow hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
    "\n",
    "#function to get word embeddings\n",
    "def get_embeddings(sentences):\n",
    "  '''return BERT-like embeddings of input text\n",
    "  Args:\n",
    "    - sentences: list of strings\n",
    "  Output:\n",
    "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
    "  '''\n",
    "  preprocessed_text = preprocessor(sentences)\n",
    "  return encoder(preprocessed_text)['pooled_output']\n",
    "\n",
    "#test function\n",
    "#get_embeddings([\n",
    "   # \"This is a test para ver el futuro of the model.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resource: https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d#98ee\n",
    "from keras import backend as K\n",
    "\n",
    "#functions to find recall\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "#functions to find precision\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "#functions to find f1 score\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "x = preprocessor(i)\n",
    "x = encoder(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
    "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
    "\n",
    "bert_model = tf.keras.Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of epochs\n",
    "n_epochs = 25\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "      tf.keras.metrics.TrueNegatives(),\n",
    "      tf.keras.metrics.FalsePositives(),\n",
    "      tf.keras.metrics.AUC(),\n",
    "      balanced_recall,\n",
    "      balanced_precision,\n",
    "      balanced_f1_score\n",
    "]\n",
    "\n",
    "#EarlyStopping callback to monitor validation loss\n",
    "#if metric doesn't improve for at least 3 epochs (patience = 3)\n",
    "    #training is interrupted and weights from epoch where the validation loss\n",
    "    #showed the best value (i.e. lowest) are restored (restore_best_weights = True)\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
    "                                                      patience = 3,\n",
    "                                                      restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "bert_model.compile(optimizer = \"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "model_fit = bert_model.fit(x_train,\n",
    "                      y_train,\n",
    "                      epochs = n_epochs,\n",
    "                      validation_data = (x_test, y_test),\n",
    "                      callbacks = [earlystop_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = list(range(1, 19+1))\n",
    "metric_list = list(model_fit.history.keys())\n",
    "num_metrics = int(len(metric_list)/2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
    "\n",
    "for i in range(0, num_metrics):\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
    "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
    "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
    "  ax[i].legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.history"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_BERT_on_OpenAI_Summarized_Text",
  "createdOn": 1682716645940,
  "creationTag": {
   "lastModifiedBy": {
    "login": "vkb6bn"
   },
   "lastModifiedOn": 1682716645940,
   "versionNumber": 0
  },
  "creator": "vkb6bn",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "modifiedBy": "vkb6bn",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
