{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find more frequent/important phrases with NLTK and YAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract n-grams from text data and then find the ones with the highest point wise mutual information (PMI). Find the words that co-occur together much more than they would to by chance.\n",
    "\n",
    "YAKE identifies most relevant key words in text by using statistical data from sinlg texts.\n",
    "\n",
    "Resources:\n",
    "- https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/\n",
    "- https://community.dataiku.com/t5/Knowledge-Base/How-to-use-spaCy-models-in-DSS/tac-p/12090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /home/dataiku/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/dataiku/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "\n",
    "import nltk\n",
    "nltk.download('genesis')\n",
    "nltk.download('punkt')\n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##KeyPhrase Extraction\n",
    "#import spacy\n",
    "#spacy==2.2.4\n",
    "#https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
    "#nltk==3.5\n",
    "#import pytextrank\n",
    "#from spacy.lang.en import English\n",
    "#from spacy.pipeline import SentenceSegmenter, EntityRecognizer, TextCategorizer\n",
    "#from spacy.pipeline.textrank import TextRank\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-7a45d96c-6689-4433-ae40-d8813b40da3c\" \n",
       "            onclick=\"_export_df('7a45d96c-6689-4433-ae40-d8813b40da3c')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"7a45d96c-6689-4433-ae40-d8813b40da3c\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SE_DOWNTIMETYPE</th>\n",
       "      <th>SE_SUBJECT_concat_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>pr bl da63 control cppcontainer go shutdown pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scheduling</td>\n",
       "      <td>bl ape2 tp statistic waiting end execution inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technical</td>\n",
       "      <td>slt test please ignore pr bl totalpowerprocess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weather</td>\n",
       "      <td>bad wather bad weather bad weather shutdown ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SE_DOWNTIMETYPE                          SE_SUBJECT_concat_cleaned\n",
       "0           Other  pr bl da63 control cppcontainer go shutdown pr...\n",
       "1      Scheduling  bl ape2 tp statistic waiting end execution inc...\n",
       "2       Technical  slt test please ignore pr bl totalpowerprocess...\n",
       "3         Weather  bad wather bad weather bad weather shutdown ac..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read recipe inputs\n",
    "Text_Cleaned = dataiku.Dataset(\"Text_Cleaned\")\n",
    "Text_Cleaned_df = Text_Cleaned.get_dataframe()\n",
    "Text_Cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downtime Type - Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split texts by downtime type\n",
    "OTHER = Text_Cleaned_df.loc[Text_Cleaned_df['SE_DOWNTIMETYPE'] == 'Other']\n",
    "#print(OTHER)\n",
    "\n",
    "OTHER_TEXT = OTHER['SE_SUBJECT_concat_cleaned']\n",
    "OTHER_TEXT = OTHER_TEXT.values[0]\n",
    "#OTHER_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resource', 'conflict'),\n",
       " ('bl', 'correlator'),\n",
       " ('pr', 'bl'),\n",
       " ('pr1', 'bl'),\n",
       " ('other', 'dv06'),\n",
       " (',', 'other'),\n",
       " (\"'\", ',')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the words that co-occur together - not by chance\n",
    "# initialize BigramCollocationFinder with variable\n",
    "OTHER_finder = BigramCollocationFinder.from_words(nltk.word_tokenize(OTHER_TEXT))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "OTHER_finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "OTHER_finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlator resource conflict\n",
      "resource conflict receive\n",
      "receive callbacks expect\n",
      "conflict receive callbacks\n",
      "failed activate component\n",
      "activate component control\n",
      "container crash array\n",
      "component control array\n",
      "container crash time\n",
      "status failed activate\n",
      "crash time handover\n",
      "mount subreflector power\n",
      "show weird status\n",
      "exception throw clustercommander.cpp\n",
      "execute method startsubscansequence\n",
      "work fdm acs\n",
      "sudden servo failure\n",
      "servo failure axis\n",
      "time handover control\n",
      "ccl handover control\n"
     ]
    }
   ],
   "source": [
    "#YAKE Key Phrase extraction\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(OTHER_TEXT)\n",
    "for kw in keywords:\n",
    "    print(kw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downtime Type - Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split texts by downtime type\n",
    "SCHEDULING = Text_Cleaned_df.loc[Text_Cleaned_df['SE_DOWNTIMETYPE'] == 'Scheduling']\n",
    "#print(SCHEDULING)\n",
    "\n",
    "SCHEDULING_TEXT = SCHEDULING['SE_SUBJECT_concat_cleaned']\n",
    "SCHEDULING_TEXT = SCHEDULING_TEXT.values[0]\n",
    "#SCHEDULING_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baseline', 'monitor'),\n",
       " ('covid-19', 'protocol'),\n",
       " ('isolate', 'mode'),\n",
       " ('night', 'aod'),\n",
       " ('solar', 'campaign'),\n",
       " ('solar', 'campaing'),\n",
       " ('high', 'phase'),\n",
       " ('execute', 'none'),\n",
       " ('handover', 'eng'),\n",
       " ('observation', 'isolate')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##find the words that co-occur together, not by chance\n",
    "# initialize BigramCollocationFinder with variable\n",
    "SCHEDULING_finder = BigramCollocationFinder.from_words(nltk.word_tokenize(SCHEDULING_TEXT))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "SCHEDULING_finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "SCHEDULING_finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project observe end\n",
      "project observe scheduling\n",
      "project observe\n",
      "project observe waiting\n",
      "observe end shift\n",
      "project observe aca\n",
      "end shift gap\n",
      "project end shift\n",
      "shift end shift\n",
      "science project observe\n",
      "end shift end\n",
      "project observe weather\n",
      "project observe sched\n",
      "observe waiting project\n",
      "observe no project\n",
      "project observe gap\n",
      "project run end\n",
      "gap end shift\n",
      "project observe family\n",
      "gap waiting project\n"
     ]
    }
   ],
   "source": [
    "#YAKE Key Phrase extraction\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(SCHEDULING_TEXT)\n",
    "for kw in keywords:\n",
    "    print(kw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downtime Type - Technical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split texts by downtime type\n",
    "TECHNICAL = Text_Cleaned_df.loc[Text_Cleaned_df['SE_DOWNTIMETYPE'] == 'Technical']\n",
    "#print(TECHNICAL)\n",
    "\n",
    "TECHNICAL_TEXT = TECHNICAL['SE_SUBJECT_concat_cleaned']\n",
    "TECHNICAL_TEXT = TECHNICAL_TEXT.values[0]\n",
    "#TECHNICAL_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corrthreadsyncguard', 'cpp:142'),\n",
       " ('ra', 'dec'),\n",
       " ('acserr', 'namevalue'),\n",
       " ('primary', 'flux'),\n",
       " ('timing', 'pulse'),\n",
       " ('trex', 'crazy'),\n",
       " ('ml2', 'went'),\n",
       " ('selection', 'criterium'),\n",
       " ('type=10000', 'code=22'),\n",
       " ('circuit', 'braker')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##find the words that co-occur together - not by chance\n",
    "# initialize BigramCollocationFinder with variable\n",
    "TECHNICAL_finder = BigramCollocationFinder.from_words(nltk.word_tokenize(TECHNICAL_TEXT))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "TECHNICAL_finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "TECHNICAL_finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aca aca aca\n",
      "aca aos aca\n",
      "aca fail aca\n",
      "fail aca fail\n",
      "issue aca aca\n",
      "aca error aca\n",
      "aca failure aca\n",
      "aca issue aca\n",
      "aca recovery aca\n",
      "datum aca cdpcs\n",
      "correlation datum aca\n",
      "aca restart aca\n",
      "aca cdpcs aca\n",
      "aca aca corr\n",
      "aos aca error\n",
      "issue aca fail\n",
      "issue aca correlator\n",
      "aca failed receive\n",
      "aca error invoke\n",
      "aca timed wait\n"
     ]
    }
   ],
   "source": [
    "#YAKE Key Phrase extraction\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(TECHNICAL_TEXT)\n",
    "for kw in keywords:\n",
    "    print(kw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downtime Type - Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split texts by downtime type\n",
    "WEATHER = Text_Cleaned_df.loc[Text_Cleaned_df['SE_DOWNTIMETYPE'] == 'Weather']\n",
    "#print(WEATHER)\n",
    "\n",
    "WEATHER_TEXT = WEATHER['SE_SUBJECT_concat_cleaned']\n",
    "WEATHER_TEXT = WEATHER_TEXT.values[0]\n",
    "#WEATHER_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 'vapor'),\n",
       " ('mix', 'lite'),\n",
       " ('da', 'dv'),\n",
       " ('dvs', 'das'),\n",
       " ('little', 'hr'),\n",
       " ('guards', 'inform'),\n",
       " ('melt', 'little'),\n",
       " ('leave', 'track'),\n",
       " ('low', 'frequency'),\n",
       " ('mounts', 'simulation')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##find the words that co-occur together - not by chance\n",
    "# initialize BigramCollocationFinder with variable\n",
    "WEATHER_finder = BigramCollocationFinder.from_words(nltk.word_tokenize(WEATHER_TEXT))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "WEATHER_finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "WEATHER_finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wind high wind\n",
      "high wind aos\n",
      "high wind speed\n",
      "wind speed wind\n",
      "wind speed aos\n",
      "high wind high\n",
      "aos high wind\n",
      "wind wind wind\n",
      "wind aos high\n",
      "high wind wind\n",
      "wind wind speed\n",
      "wind speed high\n",
      "speed wind speed\n",
      "speed high wind\n",
      "antenna high wind\n",
      "weather high wind\n",
      "wind speed alarm\n",
      "high wind antenna\n",
      "wind wind high\n",
      "wind aos wind\n"
     ]
    }
   ],
   "source": [
    "#YAKE Key Phrase extraction\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(WEATHER_TEXT)\n",
    "for kw in keywords:\n",
    "    print(kw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "# Dataset n-grams renamed to KeyPhraseExtraction by vkb6bn on 2023-03-08 17:31:42\n",
    "n_grams = dataiku.Dataset(\"KeyPhraseExtraction\")\n",
    "#KeyPhrases.write_with_schema(n_grams_df)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_n-grams",
  "createdOn": 1677003535091,
  "creationTag": {
   "lastModifiedBy": {
    "login": "vkb6bn"
   },
   "lastModifiedOn": 1677003535091,
   "versionNumber": 0
  },
  "creator": "vkb6bn",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "modifiedBy": "vkb6bn",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
